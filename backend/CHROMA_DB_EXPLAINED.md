# Chroma DB in RAG Pipeline - Explained

## What is Chroma DB?

**Chroma DB is a vector database** - it stores embeddings (vectors) along with the original text chunks and metadata.

## How Chroma DB Works in RAG

### During Fabric Creation (Build Phase):

1. **Documents are chunked** → Text split into smaller pieces
2. **Embeddings are generated** → Each chunk is converted to a vector (using OpenAI/Azure OpenAI/HuggingFace)
3. **Stored in Chroma DB** → Vectors + original text + metadata are stored together
   ```
   Chroma DB stores:
   - Embedding vector: [0.123, -0.456, 0.789, ...] (1536 numbers)
   - Original text: "Network connectivity issue affecting multiple users..."
   - Metadata: {table: "incident", sys_id: "abc123", link: "..."}
   ```

### During Chat (Query Phase):

1. **User asks a question** → "How do I fix network issues?"
2. **Question is embedded** → Converted to a vector using the same model
3. **Chroma DB searches** → Finds chunks with similar vectors (semantic similarity)
4. **Retrieves relevant chunks** → Returns top 5 most similar chunks
5. **LLM generates response** → Uses retrieved chunks as context

## Why Chroma DB is Essential

### Without Chroma DB:
- ❌ No way to find relevant information quickly
- ❌ Would need to search through all text (slow, keyword-based)
- ❌ Can't do semantic search (finding meaning, not just keywords)

### With Chroma DB:
- ✅ Fast semantic search (finds meaning, not just keywords)
- ✅ Scales to millions of chunks
- ✅ Returns most relevant information for any question
- ✅ Enables RAG (Retrieval-Augmented Generation)

## The Complete RAG Flow

```
User Question: "How do I fix network issues?"
         ↓
Generate Embedding: [0.1, 0.2, -0.3, ...]
         ↓
Query Chroma DB: "Find chunks similar to this vector"
         ↓
Chroma DB Returns: Top 5 most relevant chunks
  - "Network connectivity issue affecting multiple users..."
  - "Printer not responding in Marketing department..."
  - "Email delivery delays reported..."
         ↓
Build Context: Combine retrieved chunks
         ↓
LLM Response: Uses context to answer question
```

## Important Notes

1. **Chroma DB doesn't generate embeddings** - it only stores them
2. **Embeddings are generated by** OpenAI/Azure OpenAI/HuggingFace models
3. **Chroma DB enables similarity search** - finding semantically similar content
4. **Without Chroma DB, RAG wouldn't work** - you'd have no way to retrieve relevant context

## Current Issue

The Azure OpenAI connection errors mean:
- Embeddings are being generated with fallback (HuggingFace)
- These embeddings are still stored in Chroma DB
- Chroma DB is working correctly
- The issue is with embedding generation, not Chroma DB

## Fix the Embedding Issue

To get better embeddings (and better search results):
1. Fix Azure OpenAI credentials in `.env`
2. OR use OpenAI API key instead
3. OR use HuggingFace (works but may be less accurate)

Chroma DB will work with any embeddings - it just stores and searches them!

